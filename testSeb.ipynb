{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "#  CELL 1: Imports & Setup\n",
    "# ============================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN = [0.4914, 0.4822, 0.4465]\n",
    "STD  = [0.2023, 0.1994, 0.2010]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(MEAN, STD),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(MEAN, STD),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root=\"../cifar-10-python\", train=True,\n",
    "                                 download=True, transform=train_transform)\n",
    "test_dataset  = datasets.CIFAR10(root=\"../cifar-10-python\", train=False,\n",
    "                                 download=True, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True,  num_workers=2)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=64, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved model param count: 4,791,314 (<= 5,000,000)\n"
     ]
    }
   ],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Enhanced Basic Block with SE (Squeeze-and-Excitation) attention\n",
    "    \"\"\"\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3,\n",
    "            stride=stride, padding=1, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(\n",
    "            planes, planes, kernel_size=3,\n",
    "            stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        # SE attention module\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(planes, planes // 16, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(planes // 16, planes, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes * self.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_planes, planes * self.expansion,\n",
    "                    kernel_size=1, stride=stride, bias=False\n",
    "                ),\n",
    "                nn.BatchNorm2d(planes * self.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        \n",
    "        # Apply SE attention\n",
    "        out = out * self.se(out)\n",
    "        \n",
    "        out += self.shortcut(x)\n",
    "        return F.relu(out)\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Improved ResNet with slightly deeper architecture and dropout\n",
    "    \"\"\"\n",
    "    def __init__(self, block, num_blocks, num_classes=10, dropout_rate=0.1):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 32\n",
    "        \n",
    "        # Initial convolution with more filters\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        # Deeper layers - slightly increase depth (2,2,2,2)\n",
    "        self.layer1 = self._make_layer(block, 32, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 64, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 128, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 256, num_blocks[3], stride=2)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Add dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc = nn.Linear(256 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks - 1)\n",
    "        layers = []\n",
    "        for s in strides:\n",
    "            layers.append(block(self.in_planes, planes, s))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "def ImprovedResNet():\n",
    "    # Increase depth slightly (2,2,2,2) instead of (1,1,1,1)\n",
    "    return ResNet(BasicBlock, [4,4,4,3], num_classes=10, dropout_rate=0.2)\n",
    "\n",
    "# Calculate parameter count\n",
    "model = ImprovedResNet().to(device)\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Improved model param count: {num_params:,} (<= 5,000,000)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            # Move your data to GPU\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * imgs.size(0)\n",
    "            _, pred = outputs.max(1)\n",
    "            correct += pred.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100.0 * correct / total\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy (Best Model): 96.13%\n"
     ]
    }
   ],
   "source": [
    "best_model = ImprovedResNet().to(device)\n",
    "# best_model.load_state_dict(torch.load(\"best_model_dropoutIncrease_augmentation.pth\"))\n",
    "best_model.load_state_dict(torch.load(\"resnet_model_200epoch_mixcut.pth\"))\n",
    "best_model.eval()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "final_loss, final_acc = evaluate(best_model, test_loader, criterion)\n",
    "print(f\"Final Test Accuracy (Best Model): {final_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_test_path = \"../cifar_test_nolabel.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved as 'submission_confidence_seb_350_samridh.csv' with 10000 entries.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    with open(custom_test_path, 'rb') as f:\n",
    "        custom_test_data = pickle.load(f)\n",
    "\n",
    "    # 'b'data'' might be shape (N, 3*32*32)\n",
    "    # 'b'ids''  is a list/array of IDs for each image\n",
    "    def custom_transform(img_np):\n",
    "        # reshape if necessary\n",
    "        if img_np.shape == (3*32*32,):\n",
    "            img_np = img_np.reshape(3,32,32).transpose(1,2,0)\n",
    "        # Convert to float tensor, scale to [0,1]\n",
    "        tensor_img = torch.from_numpy(img_np.transpose(2,0,1)).float() / 255.0\n",
    "        # Apply same normalization as CIFAR-10\n",
    "        for i in range(3):\n",
    "            tensor_img[i,:,:] = (tensor_img[i,:,:] - MEAN[i]) / STD[i]\n",
    "        return tensor_img\n",
    "\n",
    "    unlabeled_imgs = custom_test_data[b'data']\n",
    "    unlabeled_ids  = custom_test_data[b'ids']\n",
    "\n",
    "    best_model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(unlabeled_imgs)):\n",
    "            img_tensor = custom_transform(unlabeled_imgs[i]).unsqueeze(0).to(device)\n",
    "            output = best_model(img_tensor)\n",
    "            _, pred_label = torch.max(output, 1)\n",
    "            predictions.append((unlabeled_ids[i], pred_label.item()))\n",
    "\n",
    "    # Write CSV\n",
    "    csv_filename = \"submission_confidence_seb_200.csv\"\n",
    "    with open(csv_filename, 'w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow([\"ID\", \"Labels\"])  # columns\n",
    "        for (img_id, label) in predictions:\n",
    "            writer.writerow([img_id, label])\n",
    "\n",
    "    print(f\"CSV saved as '{csv_filename}' with {len(predictions)} entries.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"File '{custom_test_path}' not found. Please upload your custom test dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
